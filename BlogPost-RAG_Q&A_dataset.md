# **Riassunto tecnico del blogpost**

Il post descrive **come progettare un dataset avanzato per valutare sistemi RAG (Retrieval-Augmented Generation)**, con focus su domande multi-hop e wide. L‚Äôobiettivo √® costruire dataset che riflettano difficolt√† reali e non semplificate come quelle presenti in molti benchmark pubblici.

---

## **Perch√© servono dataset ad hoc**

I benchmark pubblici pi√π diffusi (LegalBench, MultiHop-RAG, LoCoMo) non sempre rispecchiano i casi d‚Äôuso reali dei clienti. Due problemi principali:

1. **Assenza di multi-hop complessi**, che richiedono pi√π step di reasoning.
2. **Schemi non portabili**, spesso chunk-based o con annotazioni non granulari.

Per valutare bene un sistema RAG servono dataset che:

* includano ragionamento a catena,
* siano indipendenti dalla strategia di chunking,
* offrano riferimenti riproducibili nel testo originale,
* abbiano domande di difficolt√† controllata.

---

### üìå **Che cosa hanno creato, esattamente?**

Hanno creato un **dataset di Q&A** (domande, risposte ed evidenze testuali) da usare *per valutare* la qualit√† di un sistema RAG ‚Äî non la knowledge base stessa.

Ci sono due "tipi" di dataset in gioco, ma il blogpost si concentra solo sul secondo:

---

# ‚úÖ **1. La Knowledge Base (KB) ‚Üí i documenti sorgente**

Questa *non* √® il contributo principale del blogpost.
√à semplicemente il **corpus originale** che una RAG dovrebbe usare per rispondere.

Nel caso del dataset pubblico, si tratta di:

* **20 file markdown** estratti dal **D&D SRD 5.2.1**

Questi file rappresentano *le regole di D&D*.

**Non** hanno creato questo contenuto: l‚Äôhanno solo normalizzato e preparato in markdown.

---

# ‚úÖ **2. Il dataset di *valutazione* ‚Üí domande, risposte e span**

√à questo **il dataset reale che loro hanno progettato**.

Contiene:

### üü¶ **Domande**

* Easy ‚Üí generate automaticamente su singoli documenti
* Medium ‚Üí scritte da esperti (multi-hop e wide)

### üü© **Risposte**

Generate tramite:

* LLM + Claude Skills (per multi-hop)
* LLM Retriever (per domande wide)

### üüß **Passaggi/Evidenze**

Per ogni domanda, ci sono:

* i documenti da cui √® tratta la risposta,
* gli **intervalli di testo (start_char, end_char)**,
* lo **span estratto**.

Queste evidenze servono per verificare se una RAG recupera le parti giuste di testo.

üí° **√à qui l‚Äôinnovazione: la struttura char-based rende tutto chunk-agnostic.**

---

# üéØ **In breve: cosa hanno creato?**

üëâ **Un dataset Q&A di evaluation**, NON la knowledge base.
Contiene:

* **domande**
* **risposte corrette**
* **evidenze (span) per verificare retrieval e reasoning**

La KB √® solo il materiale di partenza su cui costruiscono le domande.

---

# üìå Perch√© serve questo dataset?

Per testare oggettivamente sistemi RAG su:

* multi-hop complessi,
* domande che coinvolgono pi√π documenti,
* recupero di span testuali corretti,
* reasoning basato su fonti reali.

---

# üîç Se vuoi un‚Äôanalogia:

* La **KB** = il libro di testo.
* Il **dataset di evaluation** = l‚Äôesame con domande, risposte corrette e pagine da cui arrivano.

Il blogpost parla dell'**esame**, non del libro.

---

## **La soluzione: dataset interni + uno pubblico (D&D SRD 5.2.1)**

Il team ha creato:

* due dataset interni (domini coperti da NDA),
* un dataset **pubblico** basato su D&D SRD 5.2.1,
* tutti con struttura **char-based** (chunk-agnostic).

### **Caratteristiche della struttura**

Ogni entry contiene:

* ID
* domanda
* risposta
* passaggi/evidenze (ognuno con documento, start_char, end_char, contenuto estratto)

La struttura **char-based** permette:

* riproducibilit√†,
* indipendenza dal chunking,
* verifica precisa degli span usati dal modello.

---

## **Generazione delle domande**

### **1. Livello Easy**

Automatica:

* si prende un documento markdown casuale,
* si chiede a un LLM di generare una domanda puntuale su quel documento,
* si evitano duplicati passando le domande gi√† create finora.

Limite: **non genera domande cross-documento**.

### **2. Livello Medium**

Manuale + assistita:

* interviene un esperto per costruire domande **multi-hop** o **wide**,
* le domande vengono scritte a mano perch√© l‚Äôintera KB nel prompt genera domande troppo facili.

Domini usati:

* D&D (flavour semplice e noto),
* due dataset proprietari.

---

## **Generazione delle risposte e delle evidenze**

### **Easy**

Pipeline automatizzata:

1. Per ogni domanda si passa:

   * la domanda,
   * il singolo documento di riferimento.
2. LLM genera risposta e passaggi.
3. L‚Äôesperto valuta (accetta, corregge o scarta).

### **Medium**

Richiedono molta pi√π complessit√†. Due categorie:

---

## **Medium Multi-hop ‚Üí uso delle Claude Skills**

Per i multi-hop complessi √® stata costruita una **Claude Skill custom** composta da:

* un manuale sintetico sull'organizzazione della KB,
* l‚Äôintera KB in markdown,
* due tool Python:

  * ricerca di pattern testuali,
  * espansione del contesto attorno ai match.

### **Workflow della Skill**

1. Accesso all‚Äôoverview della KB.
2. Ricerca iterativa nei documenti:

   * pattern,
   * documenti collegati,
   * contesti ampliati.
3. Aggregazione dei risultati.
4. Produzione di risposta + passaggi dettagliati.

**Punti di forza:** reasoning multi-hop molto robusto.
**Problema:** costo medio elevato (~2$ a domanda, picchi a 11$).

---

## **Medium Wide ‚Üí uso di LLM Retriever (stile LlamaIndex)**

Per le domande ‚Äúwide‚Äù, che richiedono aggregazione da molti documenti ma non una catena logica intricata:

### **Pipeline**

1. Esecuzione in parallelo di un LLM per ogni documento ‚Üí produce passaggi candidati.
2. Un LLM aggregatore combina le evidenze e formula la risposta finale.

**Punti di forza:** pi√π economico delle Claude Skills sulle wide.
**Limiti:** inefficace sui multi-hop (un solo passaggio di estrazione).

---

## **Controllo qualit√†**

Ogni item passa tre step:

1. **Validazione semantica**
   La risposta √® corretta e completa?

2. **Validazione span**
   Gli start_char e end_char corrispondono ai file originali?

3. **Rilevanza e difficolt√†**
   La domanda rispecchia veramente la categoria easy/medium?

Domande ambigue o generiche vengono eliminate.

---

## **Dataset pubblico: D&D SRD 5.2.1**

* 56 domande (25 easy, 31 medium)
* 20 documenti markdown
* Domande su regole e meccaniche del gioco
* Disponibile su GitHub e HuggingFace

---

## **Limiti attuali e futuri sviluppi**

1. **Passaggi obbligatori vs opzionali**
   Oggi non differenziati ‚Üí penalizza sistemi che trovano percorsi alternativi.

2. **RAG agentica avanzata**
   Si vuole testare pipeline di deep research multi-step.

3. **Domande hard**
   Un terzo livello che combini multi-hop + wide insieme.

4. **Multimodalit√†**
   Char-based non copre ancora immagini e grafici.

---

## **Conclusione**

Il blogpost presenta un metodo pratico e riproducibile per creare dataset di evaluation per RAG:

* char-based,
* con difficolt√† controllata,
* generazione semi-automatizzata,
* strumenti specializzati per multi-hop (Claude Skill) e wide (LLM Retriever).

Il dataset pubblico D&D SRD 5.2.1 √® gi√† disponibile e mira a standardizzare e rendere trasparente la valutazione delle pipeline RAG.
