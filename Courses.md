#main 
## AI Agents in LangGraph✅

1. Creazione di un agente ReAct prima manuale (dovendolo richiamare manualmente ad ogni step per passare da ragionamento ad azione successiva ecc ecc..). Alla fine vediamo come automatizzare l'agente
2. Questa lezione è la più IMPORTANTE per lang graph in quanto spiega bene prima di andare sul codice. Parla di prompt templates serve per prompt personalizzati con f string, poi parla dei tool e spiega da dov'è nata l'idea di creare lang graph (perché proprio i grafi) e fa vedere che ad ogni nodo corrisponde un agente (o funzione) mentre gli edges sono connessioni tra nodi che possono essere condizionali. Parla anche dello stato, che è accessibile da qualsiasi parte (nodo o edge) del grafo, può anche essere storato in persistance layer per eventuali restart. Vediamo anche due esempi di stato, uno con dei messaggi che si addanno (non si sovrascrivono) quando ne arrivano di nuovi e uno più articolato. Poi scrive un esempio con un agente che usa tavily. Nota che l'intero grafo sta dentro la classe agent e gli si passa lo stato (definito prima), poi si addano i vari nodes e conditional edges (ciascuno è legato ad una funzione che va creata), poi si compila il grafo per trasformarlo in un runnable. Mentre i tool si passano al model e non al graph. In questo framework, a differenza di *datapizzai*, le tool call non sono automaticamente gestite ma dobbiamo prendere la tool call ritornata dal modello llm in uno specifico attributo e "indirizzarla" verso la funzione e gestendone il ritorno
3. Focus sul search tool di Tavily, confronto con quello di duckduckgo (che non è agentico e ti torna i link poi devi gestirli tu facendo scraping con beautifulsoup) mentre quello di Tavily ti torna direttamente una risposta di un llm che si è letto le info dalle fonti e ha interpretato la risposta strutturata (che è accessibile anche da noi)
4. Focus sul long time running (streaming) e persistance (poter salvare un certo stato e ritornarci). Ricostruiamo il nostro agente esattamente come la lezione 2, con l'albero, lo stato e i vari nodi. Poi introduce il concetto di checkpoint saver (checkpointer) con un db sqlite (si possono usare db esterni, postgres, redis ecc...) e poi lo si passa al graph.compile. Dentro il checkpointer possiamo tenere traccia di diverse conversazioni usando diversi threads (ognuno col suo id). Per la streaming mode basta chiamare il grafo con .stream invece che .invoke (e passiamo pure il thread). Dopo un esempio di utilizzo infine mostra come streammare token, basato sull'evento specifico
5. Lezione che inizia come è finita la precedente ma stavolta integriamo una funzione che rimpiazzi i messaggi se hanno lo stesso id. Poi ricreiamo il tool e l'agente passando però al grafo, oltre al checkpointer, anche interrupt before action (nodo dove usiamo il tool) per richiedere approvazione manuale.Ora eseguendo il grafo vediamo che si interrompe e possiamo ottenere lo stato del grafo per quel thread. Per continuare l'esecuzione va chiamato di nuovo lo stream, con lo stesso thread ma passando none.Nota che lo stato è salvato in memoria con una sua history ed è accessibile tramite iterator e si può fare time travel, poi fa un esempio di modify state e time travel e gioca un pochino con le varie cose. Ci sono pure delle extra tips alla fine che non vengono spiegate in video.
6. Ultima lezione sull'essay writer: prima si genera un piano e si passa un research tool e poi si genera seguendo il piano. Una volta terminato si sceglie se giungere al termine o generare una critica al saggio scritto, sulla base della quale effettuare un'altra ricerca e riscrivere l'essay. Stavolta lo stato è più complesso dei semplici messaggi. Nota come utilizza pydantic per specificare il tipo di ritorno che vuole dall'oggetto tavily (in questo caso una lista di stringhe). Stavolta, per un uso più particolare, adoperiamo il tavily client invece del semplice tool predefinito. Questa lezione non l'ho seguita molto a livello di codice ma solo a livello di concetti, si genera pure interfaccina grafica. C'è un'ultima lezione puramente teorica su vari modelli di architettura agentica che si possono realizzare

## Long-Term Agentic Memory With LangGraph ✅

1. Creazione di agente basile per email assistant. Scriviamo prompt di base (anche se articolato), esempio di mail. Poi abbiamo istanziato l'agente e popolato il prompt con .format. Infine definito i tool, creato tutto insieme (senza memoria) e lanciato per provare
2. Qui implementiamo la semantic memory (relativa a fatti che si danno per certo, la più utile secondo me), in particolare definiamo tool per la gestione della memoria tramite store con modello di embedding, namespaces ecc.. Poi la proviamo mandando email che si riferisce ad una inviata prima
3. Semantic + episodic memory (implementata come few shot nel prompt). Qui usiamo un altro namespace dello stesso store per salvarci degli esempi di funzionamento esattamente come un few shots nel system prompt. Poi scriviamo una funzione che prima fa retrieve di quegli esempi nello store e poi formatta e spara tutto nel system prompt
4. Semantic + episodic + procedural memory (memoria diversa che viene salvata "when" succede qualcosa, descritto in linguaggio naturale al modello stesso)

## LangChain Chat with Your Data ✅

1. Document loading dove vediamo come ingestionare documenti con langchain e in particolare vediamo pdf, genericLoader per video youtube, url, notion. NOTA CHE QUESTO E' CIO' CHE HO VISTO DA SOLO MA I RAGAZZI HANNO AZURE PARSER CHE E' PAZZESCO OPPURE GEMINI 2.5PRO
2. Document splitting dove vediamo diversi splitter come recursiveCharacter, Character ma anche token splitting, e context aware splitting con markdownHeaderTextSplitter
3. Vector stores and embeddings abbastanza lineare dove vediamo come salvare qualcosa in vector store (chroma nello specifico) e poi vediamo similarity search. Inoltre vediamo anche dei problemi comuni di questa ricerca (che comunque ricorda emma ha detto che fa schifo ed è per questo che ci sono tante tecniche di advanced rag). Tra i problemi vediamo che la semantic similarity prende i documenti simili ma non ricerca la diversità (più fonti diverse sullo stesso argomento)
4. Vediamo subito come risolvere problemi segnalati prima con Maximum marginal relevance e metadata (con self query retriever che usa un llm). Infine vediamo la contextual compression che consiste nel ridurre le info da caricare in contesto. Alla fine ci mostra altri retriever come TF-IDF or SVM
5. Ultima parte dove si mette tutto in pratica ponendo i vari pezzi uno dietro l'altro

## Event-Driven Agentic Document Workflows with LLama index✅

1. Creazione di un workflow agentico con step e vari eventi (start, stop ecc.. e alcuni custom). Questo viene poi visualizzato e runnato in loop, poi vediamo anche branching e concurrent execution, oltre che come intercettare i vari eventi e la streaming mode
2. Aggiungiamo la RAG al nostro workflow, usando LlamaParse e un vectorstore di llama_index, oltre che un query engine basato su llm ottenuto tramite l'oggetto index dove salviamo il vector store. Inoltre è possibile salvare l'index su disco per renderlo permanente. Dopodiché integriamo la RAG in un tool e mettiamo tutto in un workflow agentico
3. Incorporiamo una versione più complessa del parsing all'interno del workflow (trasforma in lista di campi da essere riempiti e lo ritorna come json). Qui si usa ancora LlamaParse.
4. Human in the loop (rende interagibile tipo chatbot per suggerimenti sul migliorare le risposte dell'agente)
5. Use your voice (con Gradio e whisperer)