L'**entropia** misura quanta informazione si porta dietro, in media, un token.

Maggiore l'entropia, maggiore è l'informazione che il token si porta dietro e maggiore il numero di bit necessari per rappresentare quel token.

In questo contesto, la "quantità di informazione" è paragonabile alla "specificità" di un token: minore è il vocabolario usato da linguaggio, minore sarà la specificità di ogni token (ovvero porta con sé informazioni più generali, quindi meno specifiche), minore è l'entropia. Più è grande il vocabolario, più specifico sarà ciascun token.

Intuitivamente, l'entropia misura quanto è difficile predire il token successivo.

Un linguaggio con minore entropia (→ token più generici) sarà un linguaggio più “predicibile” (pensiamo al caso limite in cui un linguaggio abbia un vocabolario composto da 2 soli token → ogni volta la probabilità di predire quello corretto è il 50%).

#dpKnowledge 