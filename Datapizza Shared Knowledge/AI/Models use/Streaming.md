Lo streaming nei modelli LLM è una tecnica che permette di restituire l’output man mano che viene generato, invece di attendere l’intera risposta. È utile per migliorare l’esperienza utente quando le risposte sono lunghe, poiché mostra progressi in tempo reale.

Tipico di API OpenAI quando si attiva il parametro `stream=True`.

#dpKnowledge 