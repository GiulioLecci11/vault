Le regularization techniques sono strategie utilizzate in machine learning per migliorare la capacit√† di generalizzazione di un modello, ovvero la sua performance su dati non visti durante l'addestramento, essa in generale va a scongiurare l'overfitting
Overfitting: Quando un modello performa molto bene sui dati di addestramento ma fallisce su dati nuovi, perch√© "impara a memoria" invece di cogliere pattern generalizzabili.
Obiettivo: Bilanciare bias e varianza, migliorando la robustezza del modello.

Tecniche di Regularizzazione:
a. Penalizzazioni sui pesi: L1 e L2 Regularization
L1 Regularization (Lasso):

Penalizza la somma dei valori assoluti dei pesi.
Spinge i pesi di feature irrilevanti a zero, risultando in modelli pi√π semplici (selezione automatica delle feature).
Adatto quando ci sono molte feature, ma solo alcune sono importanti.
Augmented loss= Loss+lambda* summation(|wi|)

L2 Regularization (Ridge):

Penalizza la somma dei quadrati dei pesi.
Spinge i pesi verso valori piccoli ma non zero, riducendo la complessit√† del modello senza eliminare feature.
 Utile in modelli con molte feature correlate
Augmented loss= Loss+lambda*summation(wi^2)
Dropout
Definizione: Tecnica per il deep learning che disattiva casualmente un sottoinsieme di neuroni durante l‚Äôaddestramento.
Scopo: Evitare che il modello dipenda eccessivamente da specifici neuroni, promuovendo la diversit√† delle rappresentazioni.
Per esempio in una rete con 100 neuroni per layer, con ùëù=0.5, circa 50 neuroni per layer saranno inattivi ad ogni batch.

Batch Normalization
Definizione: Normalizza gli input di ogni layer per ogni batch durante l‚Äôaddestramento.
Scopo:
Riduce il problema dello shifting interno della covarianza (cambiamenti nei valori intermedi durante l‚Äôaddestramento).
Permette tassi di apprendimento pi√π alti e rende la convergenza pi√π stabile.
Consiste nel calcolare per ogni batch media e deviazione standard e poi sottrarre ad ogni elemento della batch la media e infine dividerlo per la deviazione standard.
Questo procedimento viene utilizzato praticamente sempre.

Early Stopping
Definizione: Interrompe l‚Äôaddestramento quando le prestazioni su un set di validazione smettono di migliorare.
Scopo: Prevenire l‚Äôoverfitting fermando l‚Äôaddestramento prima che il modello impari rumorosit√† dai dati.
Esempio:
Monitorare la loss di validazione e fermare l‚Äôaddestramento se non migliora per troppe epoche consecutive o se si entra in overfitting (validation error smette di decrescere insieme al train error ed inizia a crescere)

Data Augmentation
Definizione: Modifica i dati di input per aumentarne la diversit√† senza raccogliere nuovi dati.
Scopo: Ridurre l‚Äôoverfitting e rendere il modello pi√π robusto.
Esempio:
Per immagini: rotazioni, zoom, aggiunta di rumore.
Per testo: parafrasi, sostituzione di sinonimi.

Quando usare quale tecnica?
Se noti overfitting: Prova L2 regularization o dropout.
Se l‚Äôaddestramento √® lento o instabile: Usa batch normalization.
Se hai molti dati: Early stopping e L1 regularization.
Se non hai molti dati: Usa data augmentation.
#AI 
[[Neural Networks]]
[[Cognitive Computing Summary]]
